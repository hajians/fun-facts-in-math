\documentclass[11pt]{article}

\usepackage{sectsty} 
\usepackage{graphicx} 
\usepackage{ulem}
\usepackage{amsmath} 
\usepackage{amsfonts}
\usepackage{pgf}

\newcommand{\w}{\textbf{w}}
\newcommand{\x}{\textbf{x}} 
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}} 
\newcommand{\B}{\mathbb{B}} 
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Pro}{\mathbb{P}} 
\newcommand{\D}{\mathcal{D}}
\newcommand{\q}{\textsf{q}}
\newcommand{\Lsp}{\mathrm{L}}
\newcommand{\dx}{\text{d}x} 
\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\bo}[1]{{\mathbf #1}}
\newcommand{\Arr}[2]{
	\left(
		\begin{array}{c}
		{#1} 
		%
		\\
		%
		{#2}
		\end{array}
	\right)
}
\newcommand{\Arrtri}[3]{
	\left(
		\begin{array}{c}
		{#1} 
		%
		\\
		%
		{#2}
		\\
		%
		{#3}		
		\end{array}
	\right)
}
\newcommand{\ARR}[4]{
	\left(
		\begin{array}{c}
		{#1} 
		%
		\\
		%
		{#2}
		\\
		{#3} 
		%
		\\
		%
		{#4}		
		\end{array}
	\right)
}
\newcommand{\MAT}[4]{
	\left[
		\begin{array}{c|c}
		{#1} 
		%
		&
		%
		{#2}
		\\ \hline
		{#3} 
		%
		&
		%
		{#4}		
		\end{array}
	\right]
}
\newcommand{\MATT}[4]{
	\left[
		\begin{array}{cc}
		{#1} 
		%
		&
		%
		{#2}
		\\
		{#3} 
		%
		&
		%
		{#4}		
		\end{array}
	\right]
}

\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]

\usepackage{xcolor}

\newcommand{\red}[1]{{\color{red}#1}}
% Margins
%% \topmargin=-0.45in
%% \evensidemargin=0in
%% \oddsidemargin=0in
%% \textwidth=6.5in
%% \textheight=9.0in
%% \headsep=0.25in

\title{Fun facts in mathematics} \author{Soheil Hajian} \date{\today}

\begin{document}
\maketitle
% Optional TOC
\tableofcontents
\pagebreak

%--Paper--
\section{(Numerical) linear algebra}
\subsection{\red{Rayleigh quotient}} \label{sec:rayleigh}
\section{Probability theory}
\subsection{Chebyshev inequality}
Let $X$ be a continuous random variable with density function
$f_X(x)$, mean $\mu$ and standard deviation $\sigma$. Then the
following inequality holds
\begin{equation} \label{eq:chebyshev-ineq}
  \Pro(|X-\mu| \geq k \sigma) \leq \frac{1}{k^2}, \quad \forall k \geq
  1.
\end{equation}
In order to prove (\ref{eq:chebyshev-ineq}) we use definition of the
probability and standard deviation of $X$:
\begin{equation}
  \begin{array}{rcl}
    \Pro(|X-\mu| \geq k \sigma) &=& \int_{|x-\mu|\geq k\sigma} f_X(x)
    \dx \\ &\leq& \int_{|x-\mu|\geq k\sigma}
    \frac{|x-\mu|^2}{(k\sigma)^2} f_X(x) \dx \\ &\leq& \int_{\R}
    \frac{|x-\mu|^2}{(k\sigma)^2} f_X(x) \dx \\ &=&
    \frac{\sigma^2}{(k\sigma)^2} \\ &=& \frac{1}{k^2}.
  \end{array}
\end{equation}
%
\subsection{Law of large numbers}
Let us consider a sequence of independent and identically distributed
(i.i.d.) samples $(X_1, X_2, \dots, X_n)$ from a common random
variable $X$. The law of large numbers states that the mean of the
above sequence converges to the mean of $X$ as $n$ grows to
infinity. That is
\begin{equation}
  \bar{X}_n \rightarrow \mu \text{ as } n \rightarrow \infty,
\end{equation}
where
\begin{equation}
  \bar{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n},
\end{equation}
and $\mu = \mathbb{E}(X)$. Convergence should be understood in the
sense of almost surely (a.s.) which corresponds to the strong law of
large numbers. The weak law of large numbers corresponds to the
convergence in probability of the mean of the samples to the mean of
$X$.

Here we prove the weak law of large numbers for the case when $X$ is
continuous random variable. From Chebyshev inequality
(\ref{eq:chebyshev-ineq}) we have for the random variable $\bar{X}_n$
\begin{equation} 
  \Pro(|\bar{X}_n-\mu| \geq \varepsilon) \leq
  \frac{\sigma_{\bar{X}_n}^2}{\varepsilon^2},
\end{equation}
where we set $\varepsilon = k \sigma_{\bar{X}_n}$. On the other hand
from the definition of $\bar{X}_i$ we can conclude that
\begin{equation*}
  \sigma_{\bar{X}_n} = \frac{1}{\sqrt{n}} \sigma_X.
\end{equation*}
If $\sigma_X$ is finite we can conclude that
\begin{equation*} 
  \Pro(|\bar{X}_n-\mu| \geq \varepsilon) \leq \frac{\sigma_{X}^2}{n
    \varepsilon^2},
\end{equation*}
and therefore
\begin{equation} 
  1- \frac{\sigma_{X}^2}{n \varepsilon^2} \leq \Pro(|\bar{X}_n-\mu|
  \leq \varepsilon) \leq 1, \quad \forall \varepsilon > 0.
\end{equation}
Letting $n \rightarrow 0$ yields that $ \Pro(|\bar{X}_n-\mu| \leq
\varepsilon) \rightarrow 1$ for all $\varepsilon > 0$.

\section{Pattern recognition}
\subsection{Linear discriminant analysis}
In this section we introduce a method to find a subspace that that
aims to maximize the distance between data points belonging to
different classes (intra-classes distance) while minimizing the
distance of data points belonging to the same class (inter-classes).

Let us denote the dataset by the tuple $(X, y)$ where $X \in \R^{n
  \times p}$ is the so-called feature matrix, and $y \in \B^{p}$ is
the target vector. Here $p$ denotes the number of data points in the
dataset and $n$ is the dimension of the feature space.

Each column of $X$ corresponds to the features of a data point which
we denote by $x^{(j)} \in \R^{n}$, and similarly each entry of $y$,
i.e., $y^{(j)}$ corresponds to the class of the data point $j$, for
all $j=1,\dots, p$. We consider that each data point can belong to one
and only one class, and we denote the total number of classes by
$K$. That is $y^{(j)} \in \{1,\dots, K\}$ for all $j=1,\dots, p$.


\subsubsection{Scatter matrices and spread}
Let us denote the projection of the features of a data point,
$x^{(j)}$, into a normalized vector $\q \in \R^{n}$ by $z^{(j)}$, and
for all data points by $z := \q^\top X$. A measure of the spread of
projected values can be define by the Euclidean $2$-norm,
$\norm{z}_2$:
\begin{equation}
  \norm{z}_2 = \q^\top X X^\top \q.
\end{equation}
The matrix $X^\top X$ is called the scatter matrix and we denote it by
$S \in \R^{n\times n}$. Note that from the definition of $S$ we can
conclude that $S$ is symmetric and at least positive
semi-definite. Therefore maximizing the spread corresponds to finding
the normalized vector $\q$ that maximizes $\q^\top S \q$, which by
Rayleigh quotient (see Section \ref{sec:rayleigh}) corresponds to the
eigenvector of $S$ corresponding to the maximum eigenvalue of $S$.

We will now define two metrics: intra-class and inter-class spread of
the dataset. It is convenient to define first the set of data points
that belong to the same class. Let us define the set of data points
belonging to the same class by
\begin{equation}
  \D_k := \big\{ j : y^{(j)} = k, \quad \forall j=1,\dots,p \big\},
\end{equation}
for all $k = 1, \dots, K$. This then motivates to rearrange the
feature matrix, $X$, such that data points belonging to the same class
be adjacent to each other:
\begin{equation}
  X = [X_1, X_2, \dots, X_K],
\end{equation}
and similarly the target vector $y = (y_1, \dots, y_K)^\top$. Note
that each matrix $X_k$ can have a different size, i.e., $X_k \in \R^{n
  \times p_k}$ for all $k = 1, \dots, K$.

In order to define the intra-class spread, we first translate each
class the data points to the origin. The center of a class is defined
by
\begin{equation}
  c_k := \frac{1}{|\D_k|} \sum_{x \in \D_k} x.
\end{equation}
We then define the centered feature matrix of each class by
\begin{equation}
  X_{k, c} := \Big[x^{(j_1)}-c_k, \quad x^{(j_2)}-c_k, \quad \dots,
    \quad x^{(j_{p_k})}-c_k \Big],
\end{equation}
and the centered feature matrix by $X_w = [X_{1, c}, \dots, X_{K,
    c}]$.  The intra-class spread matrix is then defined by $S_w :=
X_w^{} X_w^{\top}$. We now focus on defining inter-class spread. Let
us define the global centroid of the data by
\begin{equation}
  c := \frac{1}{p} \sum_{i=1}^{p} x^{(i)}.
\end{equation}
We would like to measure the distance of the centroid of each cluster
to the global centroid. Therefore we define the following matrix
\begin{equation}
  \bar{X} := [ \underbrace{(c_1 - c), \dots, (c_1 - c)}_{p_1
      \text{ times}}, \dots, \underbrace{(c_k - c), \dots, (c_k
      - c)}_{p_k \text{ times}}] \in \R^{n \times p},
\end{equation}
whose columns measures the distance of each centroid to the global
centroid. Finally the inter-class spread matrix can be defined by
\begin{equation}
  S_b := \bar{X} \bar{X}^{\top}.
\end{equation}

As we discussed earlier in this section, we would like to maximize
inter-class spread while minimizing the intra-class spread. To do so
we can scalarize the above two metrics by defining the following
scalar function:
\begin{equation}
  H(\q) := \frac{ \q^{\top}_{} S_{b}^{} \q }{\q^{\top}_{}
    S_{w}^{} \q}, \quad \forall q \in \R^{n}, q \not = 0.
\end{equation}
Any direction $\q$ that maximizes $H(\q)$ can be viewed as a desirable
direction maximizes the ratio of the spread of inter-class over the
spread of the intra-class. Note that such direction does not
necessarily maximizes spread of inter-class and minimizes spread of
intra-class simultaneously.

Let us suppose for the moment that $S_w$ is symmetric positive
definite (s.p.d.). Then using Rayleigh quotient of Section
\ref{sec:rayleigh} we can conclude that the maximum value of $H(\q)$
corresponds to the maximum eigenvalue of $S_w^{-1} S_b$ and the
optimum direction is the corresponding eigenvector of $S_w^{-1} S_b$,
i.e.,
\begin{equation}
  v_{\max} = \arg \max_{\q\in \R^{n}} H(\q),
\end{equation}
where
\begin{equation}
  S_w^{-1} S_b \, v_{\max} = \lambda_{\max} \, v_{\max}.
\end{equation}
For the case where $S_w$ is only semi-definite, we cannot use the
argument that $S_w^{-1}$ exists. One approach would be to regularize
$S_w$ to an s.p.d. matrix by adding an s.p.d. matrix, e.g.,
\begin{equation}
  S_{w,\varepsilon} := S_w + \varepsilon I, \quad \varepsilon >
  0.
\end{equation}
Here $\varepsilon$ is a small positive real number.
\subsubsection{Numerical example}
In this section, we would like to demonstrate the effectiveness of LDA on an annotate data set. We consider a synthetic dataset originating from two Gaussian distribution (corresponding to two distinct classes, i.e., $K=2$) with the probability density functions 
\begin{equation}
	f_X(x) \propto e^{-\frac12 (x - \mu_{k}^{})^\top 
	\Sigma^{-1}_{k} (x - \mu_{k}^{})}, \quad k = 1, 2,
\end{equation}
with covariance matrix
\begin{equation}
	\Sigma_1 = \Sigma_2 = 
	\MATT{5}{2}{2}{1},
\end{equation}
and means
\begin{equation}
	\mu_1 = \Arr{0}{0}, \quad \mu_2 = \Arr{0}{10}.
\end{equation}
For a realization of this dataset see Figure \ref{fig:synthetic-data-lda} (left). In Figure \ref{fig:synthetic-data-lda} (right) we find the projection of the dataset in the subspace defined by $\q$. Note that the two classes are well-separated in this subspace.
\begin{figure}
    \begin{center}
        \scalebox{0.6}{\input{examples/lda/synthetic_data.pgf}}
        \scalebox{0.6}{\input{examples/lda/lda_projection.pgf}}
    \end{center}
    \caption{Synthetic dataset from two Gaussian distributions with the LDA direction $\q$ as black arrow (left), and projected dataset into the subspace defined by LDA (right).}
    \label{fig:synthetic-data-lda}
\end{figure}
\subsection{Logistic regression}
Logistic regression is considered as a simple method for the
classification problem. It is considered as a discriminative
classification method since it models the posterior probability of the
target variable directly. Let us denote as before the dataset by the
tuple $(X, y)$ where $X \in \R^{n \times p}$ is the so-called feature
matrix, and $y \in \B^{p}$ is the target vector. Here $p$ denotes the
number of data points in the dataset and $n$ is the dimension of the
feature space. Logistic regression models the posterior probability of
the target variable directly through a sigmoid function, i.e.,
\begin{equation}
  \Pro(y=1|x) = \sigma( \w^\top x + w_0), \quad \w \in \R^{n}, w_0 \in \R,
\end{equation}
and from the definition of the probabilities we have that $\Pro(y=0|x) = 1 - \Pro(y=1|x)$.

Here $\sigma(a) := 1/(1+\exp(-a))$. Note that when $\w^\top x + w_0 =
0$ then $\Pro(y=0|x) = \Pro(y=1|x) = \frac12$ and we can conclude that
the logistic regression model does not prefer one class over the other
given the input $x$. Such a $(n-1)$-dimensional hyperplane is called
the decision surface. Logistic regression is called a linear
classification method since its decision boundary is a linear function
of the input $x$.

In order to find optimal parameters of logistic regression model,
i.e., $\w$ and $w_0$, we first form a log-likelihood function over the
posterior distribution and then maximize it. More precisely for a
given logistic model (fixed $\w$ and $w_0$) the probability of
observing $(y^{(j)}, x^{(j)})$ is given by
\begin{equation}
  \Pro\big((y^{(j)}, x^{(j)})| \w, w_0\big) = 
  \Pro(y^{(j)}=1|x^{(j)})^{y^{(j)}} \cdot
  \Pro(y^{(j)}=0|x^{(j)})^{(1-y^{(j)})}.
\end{equation}
Since observations of the dataset are i.i.d.~we can conclude that the
probability of observing the dataset given the model is the
multiplication of the above likelihood function, i.e.,
\begin{equation}
  \Pro\big((y, X)| \w, w_0\big) = \prod_{j=1}^{p} \Pro\big((y^{(j)}, x^{(j)})| \w, w_0\big).
\end{equation}
\section{Kernel methods}
In this section we will go through a class of methods known as {\it kernel
methods} for regression and classification. Let us start from a
regression model defined by
\begin{equation}
  f_\w(\x) = \w^\top \phi(\x), \quad \forall \x \in \R^{d_0},
\end{equation}
where $\x$ is the original feature inputs which are transformed using a
feature map $\phi: \R^{d_0} \rightarrow \R^{d}$ and $\w \in \R^{d}$ is
the weight that completes the definition of the method. We refer to
$\R^{d}$ as the feature space while $\R^{d_0}$ is the initial input
space. In this section we assume that the feature space has a much
higher dimensionality compare to initial feature space, i.e., $d \gg
d_0$.

\begin{example} \label{example:kernel}
  Suppose that we have an image of size $16 \times 16$ pixels where
  each pixel's intensity is represented by an entry in a vector
  denoted by $\x \in \R^{d_0}$. Suppose we would like to construct
  another vector consisting of all monomials of degree upto $m$, e.g.,
  for $m=2$, we have
  \begin{equation}
    \phi(\x) = (x_1, \dots, x_{d_0}, x_1 x_2, x_1 x_3, \dots,
    x_1^2, \dots, x_{d_0}^2)^\top \in \R^{d},
  \end{equation}
  where $d = \Arr{2+  d_0 - 1}{d_0 - 1} = O(d_0^2)$. In general, if we
  are interested in arbitrary $m \in \N^+$, we have
  $d = O\big( \frac{d_0^m}{m!} \big)$.
\end{example}

Suppose we have a set of data points $\{(\x_i, t_i)\}_{i=1}^{N}$ which
we can use to fit weights, i.e., $\w$. In order to do so, let us setup
a loss (objective) function and an optimization, i.e.,
\begin{equation}
  J(\w) := \frac12 \sum_{i=1}^{N} (t_i - \w^\top \phi(\x_i))^2 +
  \frac{\lambda}{2} \norm{\w}_2^2,
\end{equation}
and find $\w^\star$ such that
\begin{equation}
  \w^\star = \arg \min_{\w \in \R^{d}} J(\w).
\end{equation}

In order to solve for the optimal weights we need to take the
derivative of $J(\w)$ with respect to its argument and set it to
zero:
\begin{equation}
  \nabla J(\w) = \lambda \w - \sum_{i=1}^{N} (t_i - \w^\top \phi(\x_i)) \phi(\x_i)
  = 0.
\end{equation}
In order to find the solution to this equation, we first define
$X:=\big[\phi(\x_1), \dots, \phi(\x_N) \big]$ and $\bo{t} := (t_1,
\dots, t_N)^\top$. Then the optimal
$\w^\star$ reads
\begin{equation}
  \big( \lambda I_d + X X^\top \big) \w^\star = X \bo{t}.
\end{equation}

On the other hand, let us rewrite the optimal weights in the following form
\begin{equation}
  \w^\star = \frac{1}{\lambda} X \bo{t} - X X^\top \w^\top
  = X \Big( \frac{1}{\lambda} \bo{t} - X^\top \w^\star \Big) =: X \bo{a},
\end{equation}
where $\bo{a} \in \R^{N}$. Substituting $\w^\star = X \bo{a}$ into
the function definition yields
\begin{equation}
  f^\star(\x) = \bo{a}^\top X^\top \phi(x) = \sum_{j=1}^{N} a_i
  \phi(\x_i)^\top \phi(x) \quad \forall \x \in \R^{d_0}.
\end{equation}
This implies that although we search in a rich function space for the
optimal weights, i.e., $\R^d$, the optimal solution lies in a much
smaller space, namely the space spanned by the training dataset
$\{\phi(\x_i)\}_{i=1}^{N}$. Often the problem is posed in a setting
where $N \ll d$, e.g., see Example \ref{example:kernel}. Therefore it
might be much cheaper to pose the original function space and
optimization in terms of $\bo{a}$. We call this representation of the
problem the {\it dual representation}.

The dual representation in terms of $\bo{a}$ is the following:
\begin{equation}
  J(\bo{a}) := \frac12 \sum_{i=1}^{N} (t_i -
  \bo{a}^\top X^\top  \phi(\x_i))^2 +
  \frac{\lambda}{2} \bo{a}^\top X^\top X \bo{a},
\end{equation}
and $\bo{a}^\star := \arg\min_{\bo{a}\in\R^N} J(\bo{a})$.
The optimal $\bo{a}^\star$ satisfies
\begin{equation}
  (\lambda I_N + X^\top X) \bo{a}^\star = \bo{t}.
\end{equation}
The solution of the primal and dual representations coincide in the
sense that
\begin{equation}
  \bo{w}^\star = X \bo{a}^\star.
\end{equation}
\begin{remark}
  The two function spaces of the primal and dual representations do
  not coincide. Observe that $\bo{a}\in\R^N$ while $\w \in \R^d$ and
  $N \ll d$. Therefore the primal representation has a richer function
  space, however the optimal solution of both formulations lies in $\R^N$.
\end{remark}
\subsection{Kernel trick}
Recall that in the dual representation the regression function is
\begin{equation}
  f_{\bo{a}}(\x) = \sum_{j=1}^{N} a_j \phi(\x_j)^\top \phi(\x).
\end{equation}
Note that we can replace $\phi(\x_i)^\top \phi(\x)$ by a {\it kernel}
function $k(\x_i, \x) = \phi(\x_i)^\top \phi(\x)$ in order to avoid
carrying around the high dimensional map $\phi(\x)$. More precisely,
we need to find a kernel function $k(\x, \bo{y})$ that admits a
decomposition $k(\x, \bo{y}) = \phi(\x)^\top \phi(\bo{y})$ for some
$\phi(\cdot)$. A necessary and sufficient condition for a valid kernel
function is that the matrix $K$, whose entries are $K_{mn} = k(\x_m,
\x_n)$ for all sets of $\{\x_i\}_{i=1}^{N}$, is positive
semi-definite. The matrix $K$ is called {\it Gram matrix}.
\begin{example}
  The kernel corresponding to the problem posed in Example
  \ref{example:kernel} is $k(\x, \x') = (\x^\top \x')^2$ for all $\x,
  \x' \in \R^{d_0}$.
\end{example}

\end{document}
