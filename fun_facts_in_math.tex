\documentclass[11pt]{article}

\usepackage{sectsty} 
\usepackage{graphicx} 
\usepackage{ulem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{pgf}
\usepackage{mathtools}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand{\w}{\textbf{w}}
\newcommand{\x}{\textbf{x}} 
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}} 
\newcommand{\B}{\mathbb{B}} 
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\Pro}{\mathbb{P}} 
\newcommand{\D}{\mathcal{D}}
\newcommand{\q}{\textsf{q}}
\newcommand{\Lsp}{\mathrm{L}}
\newcommand{\dx}{\text{d}x} 
\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\bo}[1]{{\mathbf #1}}
\newcommand{\Arr}[2]{
	\left(
		\begin{array}{c}
		{#1} 
		%
		\\
		%
		{#2}
		\end{array}
	\right)
}
\newcommand{\Arrtri}[3]{
	\left(
		\begin{array}{c}
		{#1} 
		%
		\\
		%
		{#2}
		\\
		%
		{#3}		
		\end{array}
	\right)
}
\newcommand{\ARR}[4]{
	\left(
		\begin{array}{c}
		{#1} 
		%
		\\
		%
		{#2}
		\\
		{#3} 
		%
		\\
		%
		{#4}		
		\end{array}
	\right)
}
\newcommand{\MAT}[4]{
	\left[
		\begin{array}{c|c}
		{#1} 
		%
		&
		%
		{#2}
		\\ \hline
		{#3} 
		%
		&
		%
		{#4}		
		\end{array}
	\right]
}
\newcommand{\MATT}[4]{
	\left[
		\begin{array}{cc}
		{#1} 
		%
		&
		%
		{#2}
		\\
		{#3} 
		%
		&
		%
		{#4}		
		\end{array}
	\right]
}

\newtheorem{example}{Example}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}[section]

\usepackage{xcolor}

\newcommand{\red}[1]{{\color{red}#1}}
% Margins
%% \topmargin=-0.45in
%% \evensidemargin=0in
%% \oddsidemargin=0in
%% \textwidth=6.5in
%% \textheight=9.0in
%% \headsep=0.25in

\title{Fun facts in mathematics} \author{Soheil Hajian} \date{\today}

\begin{document}
\maketitle
% Optional TOC
\tableofcontents
\pagebreak

%--Paper--
\section{(Numerical) linear algebra}
\subsection{Rayleigh quotient} \label{sec:rayleigh}
Rayleigh quotient is a quotient defined for symmetric matrices and can
be used as a technique for estimating minimum and maximum eigenvalues
of a symmetric matrix. Let $A \in \R^{n\times n}$ be a symmetric
matrix, i.e., $A=A^\top$. Then the Rayleigh quotient is defined as
\begin{equation}
  R(A, \w) := \frac{\w^\top A \w}{\w^\top \w}, \quad \forall \w \in \R^{n}.
\end{equation}
It can be shown for a symmetric matrix, $A$, and
its eigenvalues $\{\lambda\}_{i=1}^{n}$, we have
\begin{equation}
  \lambda_{\max} = \max_{\w \in \R^n} R(A, \w),
  \lambda_{\min} = \min_{\w \in \R^n} R(A, \w),
\end{equation}
and therefore
\begin{equation}
  \min_{\w \in \R^n} R(A, \w) \leq \lambda_i \leq \max_{\w \in \R^n}
  R(A, \w), \quad \forall i = 1, \dots, n.
\end{equation}
\section{Probability theory and statistics}
\subsection{Terminologies}
In this section we introduce some terminologies used in statistics.
\subsubsection{\color{red}Statistic and estimators}
A statistic or sample statistic is a quantity that is computed using a
sample of a population. An estimator is a statistic used to estimate a
{\it quantity of interest} of the population. For example the mean of a
sample from a population is a statistic that can be used as an
estimator for the mean of the population (more precisely, mean of its
probability distribution).

In the following we will introduce properties of the
estimators. Suppose we are interested in knowing a quantity of
interest which we denote by $\theta$ of a population which is
described through a random variable, $X$. For example the average
height in a population where the height of an individual in the
population is a random variable denoted by $X$. In this example $\theta
:= \Ex[X]$.

Let us denote an estimator of $\theta$ by $\hat{\theta}(X)$ (note that
$\hat{\theta}(X)$ is stochastic but given an observation $x$,
$\hat{\theta}(x)$ is a fixed value). Then the estimator $\hat{\theta}$
is said to be:
\begin{itemize}
\item Unbiased if $\Ex[\hat{\theta}] = \theta$. In our example, the
  estimator $\hat{\theta}(X) = \frac{1}{N}\sum_{i=1}^{N} X_i$ where
  $\{X_i\}_{i=1}^{N}$ are i.i.d. random samples whose probability
  densities is same as $X$ is an unbiased estimator of $\Ex[X]$.  
\item Minimum variance unbiased estimator: if the estimator is
  unbiased and its variance $\Var[\hat{\theta}] := \Ex[(\hat{\theta}
    - \Ex[\hat{\theta}])^2]$ is the smallest among all unbiased
  estimators.
  \item Consistent: if a sequence of estimators
    $\{\hat{\theta}_n\}_{n=1}^{\infty}$ converges to the quantity of
    the interest when $n \rightarrow \infty$. That is for all $\varepsilon
    >0$, we have
    \begin{equation}
      \lim_{n \rightarrow \infty} \Pro \big[ |\hat{\theta}_n - \theta| <
        \varepsilon \big] = 1.  
    \end{equation}
\end{itemize}
\subsection{Chebyshev inequality}
Let $X$ be a continuous random variable with density function
$f_X(x)$, mean $\mu$ and standard deviation $\sigma$. Then the
following inequality holds
\begin{equation} \label{eq:chebyshev-ineq}
  \Pro(|X-\mu| \geq k \sigma) \leq \frac{1}{k^2}, \quad \forall k \geq
  1.
\end{equation}
In order to prove (\ref{eq:chebyshev-ineq}) we use definition of the
probability and standard deviation of $X$:
\begin{equation}
  \begin{array}{rcl}
    \Pro(|X-\mu| \geq k \sigma) &=& \int_{|x-\mu|\geq k\sigma} f_X(x)
    \dx \\ &\leq& \int_{|x-\mu|\geq k\sigma}
    \frac{|x-\mu|^2}{(k\sigma)^2} f_X(x) \dx \\ &\leq& \int_{\R}
    \frac{|x-\mu|^2}{(k\sigma)^2} f_X(x) \dx \\ &=&
    \frac{\sigma^2}{(k\sigma)^2} \\ &=& \frac{1}{k^2}.
  \end{array}
\end{equation}
%
\subsection{Law of large numbers}
Let us consider a sequence of independent and identically distributed
(i.i.d.) samples $(X_1, X_2, \dots, X_n)$ from a common random
variable $X$. The law of large numbers states that the mean of the
above sequence converges to the mean of $X$ as $n$ grows to
infinity. That is
\begin{equation}
  \bar{X}_n \rightarrow \mu \text{ as } n \rightarrow \infty,
\end{equation}
where
\begin{equation}
  \bar{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n},
\end{equation}
and $\mu = \mathbb{E}(X)$. Convergence should be understood in the
sense of almost surely (a.s.) which corresponds to the strong law of
large numbers. The weak law of large numbers corresponds to the
convergence in probability of the mean of the samples to the mean of
$X$.

Here we prove the weak law of large numbers for the case when $X$ is
continuous random variable. From Chebyshev inequality
(\ref{eq:chebyshev-ineq}) we have for the random variable $\bar{X}_n$
\begin{equation} 
  \Pro(|\bar{X}_n-\mu| \geq \varepsilon) \leq
  \frac{\sigma_{\bar{X}_n}^2}{\varepsilon^2},
\end{equation}
where we set $\varepsilon = k \sigma_{\bar{X}_n}$. On the other hand
from the definition of $\bar{X}_i$ we can conclude that
\begin{equation*}
  \sigma_{\bar{X}_n} = \frac{1}{\sqrt{n}} \sigma_X.
\end{equation*}
If $\sigma_X$ is finite we can conclude that
\begin{equation*} 
  \Pro(|\bar{X}_n-\mu| \geq \varepsilon) \leq \frac{\sigma_{X}^2}{n
    \varepsilon^2},
\end{equation*}
and therefore
\begin{equation} 
  1- \frac{\sigma_{X}^2}{n \varepsilon^2} \leq \Pro(|\bar{X}_n-\mu|
  \leq \varepsilon) \leq 1, \quad \forall \varepsilon > 0.
\end{equation}
Letting $n \rightarrow \infty$ yields that $ \Pro(|\bar{X}_n-\mu| \leq
\varepsilon) \rightarrow 1$ for all $\varepsilon > 0$.

\section{Pattern recognition}
\subsection{Linear discriminant analysis}
In this section we introduce a method to find a subspace that that
aims to maximize the distance between data points belonging to
different classes (intra-classes distance) while minimizing the
distance of data points belonging to the same class (inter-classes).

Let us denote the dataset by the tuple $(X, y)$ where $X \in \R^{n
  \times p}$ is the so-called feature matrix, and $y \in \B^{p}$ is
the target vector. Here $p$ denotes the number of data points in the
dataset and $n$ is the dimension of the feature space.

Each column of $X$ corresponds to the features of a data point which
we denote by $x^{(j)} \in \R^{n}$, and similarly each entry of $y$,
i.e., $y^{(j)}$ corresponds to the class of the data point $j$, for
all $j=1,\dots, p$. We consider that each data point can belong to one
and only one class, and we denote the total number of classes by
$K$. That is $y^{(j)} \in \{1,\dots, K\}$ for all $j=1,\dots, p$.

\subsubsection{Scatter matrices and spread}
Let us denote the projection of the features of a data point,
$x^{(j)}$, into a normalized vector $\q \in \R^{n}$ by $z^{(j)}$, and
for all data points by $z := \q^\top X$. A measure of the spread of
projected values can be define by the Euclidean $2$-norm,
$\norm{z}_2$:
\begin{equation}
  \norm{z}_2 = \q^\top X X^\top \q.
\end{equation}
The matrix $X^\top X$ is called the scatter matrix and we denote it by
$S \in \R^{n\times n}$. Note that from the definition of $S$ we can
conclude that $S$ is symmetric and at least positive
semi-definite. Therefore maximizing the spread corresponds to finding
the normalized vector $\q$ that maximizes $\q^\top S \q$, which by
Rayleigh quotient (see Section \ref{sec:rayleigh}) corresponds to the
eigenvector of $S$ corresponding to the maximum eigenvalue of $S$.

We will now define two metrics: intra-class and inter-class spread of
the dataset. It is convenient to define first the set of data points
that belong to the same class. Let us define the set of data points
belonging to the same class by
\begin{equation}
  \D_k := \big\{ j : y^{(j)} = k, \quad \forall j=1,\dots,p \big\},
\end{equation}
for all $k = 1, \dots, K$. This then motivates to rearrange the
feature matrix, $X$, such that data points belonging to the same class
be adjacent to each other:
\begin{equation}
  X = [X_1, X_2, \dots, X_K],
\end{equation}
and similarly the target vector $y = (y_1, \dots, y_K)^\top$. Note
that each matrix $X_k$ can have a different size, i.e., $X_k \in \R^{n
  \times p_k}$ for all $k = 1, \dots, K$.

In order to define the intra-class spread, we first translate each
class the data points to the origin. The center of a class is defined
by
\begin{equation}
  c_k := \frac{1}{|\D_k|} \sum_{x \in \D_k} x.
\end{equation}
We then define the centered feature matrix of each class by
\begin{equation}
  X_{k, c} := \Big[x^{(j_1)}-c_k, \quad x^{(j_2)}-c_k, \quad \dots,
    \quad x^{(j_{p_k})}-c_k \Big],
\end{equation}
and the centered feature matrix by $X_w = [X_{1, c}, \dots, X_{K,
    c}]$.  The intra-class spread matrix is then defined by $S_w :=
X_w^{} X_w^{\top}$. We now focus on defining inter-class spread. Let
us define the global centroid of the data by
\begin{equation}
  c := \frac{1}{p} \sum_{i=1}^{p} x^{(i)}.
\end{equation}
We would like to measure the distance of the centroid of each cluster
to the global centroid. Therefore we define the following matrix
\begin{equation}
  \bar{X} := [ \underbrace{(c_1 - c), \dots, (c_1 - c)}_{p_1
      \text{ times}}, \dots, \underbrace{(c_k - c), \dots, (c_k
      - c)}_{p_k \text{ times}}] \in \R^{n \times p},
\end{equation}
whose columns measures the distance of each centroid to the global
centroid. Finally the inter-class spread matrix can be defined by
\begin{equation}
  S_b := \bar{X} \bar{X}^{\top}.
\end{equation}

As we discussed earlier in this section, we would like to maximize
inter-class spread while minimizing the intra-class spread. To do so
we can scalarize the above two metrics by defining the following
scalar function:
\begin{equation}
  H(\q) := \frac{ \q^{\top}_{} S_{b}^{} \q }{\q^{\top}_{}
    S_{w}^{} \q}, \quad \forall q \in \R^{n}, q \not = 0.
\end{equation}
Any direction $\q$ that maximizes $H(\q)$ can be viewed as a desirable
direction maximizes the ratio of the spread of inter-class over the
spread of the intra-class. Note that such direction does not
necessarily maximizes spread of inter-class and minimizes spread of
intra-class simultaneously.

Let us suppose for the moment that $S_w$ is symmetric positive
definite (s.p.d.). Then using Rayleigh quotient of Section
\ref{sec:rayleigh} we can conclude that the maximum value of $H(\q)$
corresponds to the maximum eigenvalue of $S_w^{-1} S_b$ and the
optimum direction is the corresponding eigenvector of $S_w^{-1} S_b$,
i.e.,
\begin{equation}
  v_{\max} = \arg \max_{\q\in \R^{n}} H(\q),
\end{equation}
where
\begin{equation}
  S_w^{-1} S_b \, v_{\max} = \lambda_{\max} \, v_{\max}.
\end{equation}
For the case where $S_w$ is only semi-definite, we cannot use the
argument that $S_w^{-1}$ exists. One approach would be to regularize
$S_w$ to an s.p.d. matrix by adding an s.p.d. matrix, e.g.,
\begin{equation}
  S_{w,\varepsilon} := S_w + \varepsilon I, \quad \varepsilon >
  0.
\end{equation}
Here $\varepsilon$ is a small positive real number.
\subsubsection{Numerical example}
In this section, we would like to demonstrate the effectiveness of LDA on an annotate data set. We consider a synthetic dataset originating from two Gaussian distribution (corresponding to two distinct classes, i.e., $K=2$) with the probability density functions 
\begin{equation}
	f_X(x) \propto e^{-\frac12 (x - \mu_{k}^{})^\top 
	\Sigma^{-1}_{k} (x - \mu_{k}^{})}, \quad k = 1, 2,
\end{equation}
with covariance matrix
\begin{equation}
	\Sigma_1 = \Sigma_2 = 
	\MATT{5}{2}{2}{1},
\end{equation}
and means
\begin{equation}
	\mu_1 = \Arr{0}{0}, \quad \mu_2 = \Arr{0}{10}.
\end{equation}
For a realization of this dataset see Figure \ref{fig:synthetic-data-lda} (left). In Figure \ref{fig:synthetic-data-lda} (right) we find the projection of the dataset in the subspace defined by $\q$. Note that the two classes are well-separated in this subspace.
\begin{figure}
    \begin{center}
        \scalebox{0.6}{\input{examples/lda/synthetic_data.pgf}}
        \scalebox{0.6}{\input{examples/lda/lda_projection.pgf}}
    \end{center}
    \caption{Synthetic dataset from two Gaussian distributions with the LDA direction $\q$ as black arrow (left), and projected dataset into the subspace defined by LDA (right).}
    \label{fig:synthetic-data-lda}
\end{figure}
\subsection{Logistic regression}
Logistic regression is considered as a simple method for the
classification problem. It is considered as a discriminative
classification method since it models the posterior probability of the
target variable directly. Let us denote as before the dataset by the
tuple $(X, y)$ where $X \in \R^{n \times p}$ is the so-called feature
matrix, and $y \in \B^{p}$ is the target vector. Here $p$ denotes the
number of data points in the dataset and $n$ is the dimension of the
feature space. Logistic regression models the posterior probability of
the target variable directly through a sigmoid function, i.e.,
\begin{equation}
  \Pro(y=1|x) = \sigma( \w^\top x + w_0), \quad \w \in \R^{n}, w_0 \in \R,
\end{equation}
and from the definition of the probabilities we have that $\Pro(y=0|x) = 1 - \Pro(y=1|x)$.

Here $\sigma(a) := 1/(1+\exp(-a))$. Note that when $\w^\top x + w_0 =
0$ then $\Pro(y=0|x) = \Pro(y=1|x) = \frac12$ and we can conclude that
the logistic regression model does not prefer one class over the other
given the input $x$. Such a $(n-1)$-dimensional hyperplane is called
the decision surface. Logistic regression is called a linear
classification method since its decision boundary is a linear function
of the input $x$.

In order to find optimal parameters of logistic regression model,
i.e., $\w$ and $w_0$, we first form a log-likelihood function over the
posterior distribution and then maximize it. More precisely for a
given logistic model (fixed $\w$ and $w_0$) the probability of
observing $(y^{(j)}, x^{(j)})$ is given by
\begin{equation}
  \Pro\big((y^{(j)}, x^{(j)})| \w, w_0\big) = 
  \Pro(y^{(j)}=1|x^{(j)})^{y^{(j)}} \cdot
  \Pro(y^{(j)}=0|x^{(j)})^{(1-y^{(j)})}.
\end{equation}
Since observations of the dataset are i.i.d.~we can conclude that the
probability of observing the dataset given the model is the
multiplication of the above likelihood function, i.e.,
\begin{equation}
  \Pro\big((y, X)| \w, w_0\big) = \prod_{j=1}^{p} \Pro\big((y^{(j)}, x^{(j)})| \w, w_0\big).
\end{equation}
\subsection{Kernel methods}
In this section we will go through a class of methods known as {\it kernel
methods} for regression and classification. Let us start from a
regression model defined by
\begin{equation}
  f_\w(\x) = \w^\top \phi(\x), \quad \forall \x \in \R^{d_0},
\end{equation}
where $\x$ is the original feature inputs which are transformed using a
feature map $\phi: \R^{d_0} \rightarrow \R^{d}$ and $\w \in \R^{d}$ is
the weight that completes the definition of the method. We refer to
$\R^{d}$ as the feature space while $\R^{d_0}$ is the initial input
space. In this section we assume that the feature space has a much
higher dimensionality compare to initial feature space, i.e., $d \gg
d_0$.

\begin{example} \label{example:kernel}
  Suppose that we have an image of size $16 \times 16$ pixels where
  each pixel's intensity is represented by an entry in a vector
  denoted by $\x \in \R^{d_0}$. Suppose we would like to construct
  another vector consisting of all monomials of degree upto $m$, e.g.,
  for $m=2$, we have
  \begin{equation}
    \phi(\x) = (x_1, \dots, x_{d_0}, x_1 x_2, x_1 x_3, \dots,
    x_1^2, \dots, x_{d_0}^2)^\top \in \R^{d},
  \end{equation}
  where $d = \Arr{2+  d_0 - 1}{d_0 - 1} = O(d_0^2)$. In general, if we
  are interested in arbitrary $m \in \N^+$, we have
  $d = O\big( \frac{d_0^m}{m!} \big)$.
\end{example}

Suppose we have a set of data points $\{(\x_i, t_i)\}_{i=1}^{N}$ which
we can use to fit weights, i.e., $\w$. In order to do so, let us setup
a loss (objective) function and an optimization, i.e.,
\begin{equation}
  J(\w) := \frac12 \sum_{i=1}^{N} (t_i - \w^\top \phi(\x_i))^2 +
  \frac{\lambda}{2} \norm{\w}_2^2,
\end{equation}
and find $\w^\star$ such that
\begin{equation}
  \w^\star = \arg \min_{\w \in \R^{d}} J(\w).
\end{equation}

In order to solve for the optimal weights we need to take the
derivative of $J(\w)$ with respect to its argument and set it to
zero:
\begin{equation}
  \nabla J(\w) = \lambda \w - \sum_{i=1}^{N} (t_i - \w^\top \phi(\x_i)) \phi(\x_i)
  = 0.
\end{equation}
In order to find the solution to this equation, we first define
$X:=\big[\phi(\x_1), \dots, \phi(\x_N) \big]$ and $\bo{t} := (t_1,
\dots, t_N)^\top$. Then the optimal
$\w^\star$ reads
\begin{equation}
  \big( \lambda I_d + X X^\top \big) \w^\star = X \bo{t}.
\end{equation}

On the other hand, let us rewrite the optimal weights in the following form
\begin{equation}
  \w^\star = \frac{1}{\lambda} X \bo{t} - X X^\top \w^\top
  = X \Big( \frac{1}{\lambda} \bo{t} - X^\top \w^\star \Big) =: X \bo{a},
\end{equation}
where $\bo{a} \in \R^{N}$. Substituting $\w^\star = X \bo{a}$ into
the function definition yields
\begin{equation}
  f^\star(\x) = \bo{a}^\top X^\top \phi(x) = \sum_{j=1}^{N} a_i
  \phi(\x_i)^\top \phi(x) \quad \forall \x \in \R^{d_0}.
\end{equation}
This implies that although we search in a rich function space for the
optimal weights, i.e., $\R^d$, the optimal solution lies in a much
smaller space, namely the space spanned by the training dataset
$\{\phi(\x_i)\}_{i=1}^{N}$. Often the problem is posed in a setting
where $N \ll d$, e.g., see Example \ref{example:kernel}. Therefore it
might be much cheaper to pose the original function space and
optimization in terms of $\bo{a}$. We call this representation of the
problem the {\it dual representation}.

The dual representation in terms of $\bo{a}$ is the following:
\begin{equation}
  J(\bo{a}) := \frac12 \sum_{i=1}^{N} (t_i -
  \bo{a}^\top X^\top  \phi(\x_i))^2 +
  \frac{\lambda}{2} \bo{a}^\top X^\top X \bo{a},
\end{equation}
and $\bo{a}^\star := \arg\min_{\bo{a}\in\R^N} J(\bo{a})$.
The optimal $\bo{a}^\star$ satisfies
\begin{equation}
  (\lambda I_N + X^\top X) \bo{a}^\star = \bo{t}.
\end{equation}
The solution of the primal and dual representations coincide in the
sense that
\begin{equation}
  \bo{w}^\star = X \bo{a}^\star.
\end{equation}
\begin{remark}
  The two function spaces of the primal and dual representations do
  not coincide. Observe that $\bo{a}\in\R^N$ while $\w \in \R^d$ and
  $N \ll d$. Therefore the primal representation has a richer function
  space, however the optimal solution of both formulations lies in $\R^N$.
\end{remark}
\subsubsection{Kernel trick}
Recall that in the dual representation the regression function is
\begin{equation}
  f_{\bo{a}}(\x) = \sum_{j=1}^{N} a_j \phi(\x_j)^\top \phi(\x).
\end{equation}
Note that we can replace $\phi(\x_i)^\top \phi(\x)$ by a {\it kernel}
function $k(\x_i, \x) = \phi(\x_i)^\top \phi(\x)$ in order to avoid
carrying around the high dimensional map $\phi(\x)$. More precisely,
we need to find a kernel function $k(\x, \bo{y})$ that admits a
decomposition $k(\x, \bo{y}) = \phi(\x)^\top \phi(\bo{y})$ for some
$\phi(\cdot)$. A necessary and sufficient condition for a valid kernel
function is that the matrix $K$, whose entries are $K_{mn} = k(\x_m,
\x_n)$ for all sets of $\{\x_i\}_{i=1}^{N}$, is positive
semi-definite. The matrix $K$ is called {\it Gram matrix}.
\begin{example}
  The kernel corresponding to the problem posed in Example
  \ref{example:kernel} is $k(\x, \x') = (\x^\top \x')^2$ for all $\x,
  \x' \in \R^{d_0}$.
\end{example}

\subsection{Conformal prediction}
A classification method of type $f(\x) \in \R^{n}$ generates point
estimates which approximates $\Pro(Y=k
|X=\x)$ for $q=1,\dots,K$. In
situations where assigning a class to an input is not easy, e.g., an
image classification when the image is not clear, we might attempt to
generate a set of possible classes. {\it Conformal prediction} method
attempts to generate such sets from the output of a model. In this
respect conformal prediction is a model agnostic method.

More precisely, for a given test data point $(\x_\text{test},
y_\text{test})$, where $\x_\text{test} \in \R^n$ and $y_\text{text}
\in \{1, \dots, K\}$, we would like to construct a set $C(X_\text{test})$
such that with high probability $y_\text{test} \in
C(X_\text{test})$. In the following we provide an algorithm for
generating such a $C(X_\text{test})$:
\begin{enumerate}
  \item Given a calibration set $\{(\x_i, y_i)\}_{i=1}^{p}$, a test
    data point $(\x_\text{test},
    y_\text{test})$ and a parameter $\alpha$.
  \item Compute error terms (also called conformal scores) $s_i = 1 -
    f_{y_i}(\x_i)$ and their $\frac{\ceil{(1+p)(1-\alpha)}}{p}$
    quantile denoted by $\hat{q}$.
  \item Construct the set of potential classes by
    \begin{equation*}
      C(\x_\text{test}) = \{i : 1- f_i(\x_\text{test}) \leq \hat{q}, \quad 
      \forall i = 1, \dots, K\}.
    \end{equation*}
\end{enumerate}
%% 
We will show in this section that the above procedure produces
conformal sets that contain the true class with the following
probability estimates:
\begin{equation*}
  1 - \alpha \leq \Pro\big(y_\text{test} \in C(\x_\text{test})\big) \leq
  1 - \alpha + \frac{1}{n+1}.
\end{equation*}

\begin{theorem} Suppose the calibration set $\{(\x_i,
  y_i)\}_{i=1}^{p}$ and the test data point $(\x_\text{test},
  y_\text{test})$ are i.i.d. Moreover, let $C(\x_\text{test})$ be
  generated with the above procedure. Then the probability of the true
  class be in $C(\x_\text{test})$ is given by
  $$ \Pro\big(y_\text{test} \in C(\x_\text{test})\big) =
  \frac{\ceil{(1+p)(1-\alpha)}}{p+1}.
  $$
\end{theorem}
\begin{proof}
  Let us, without loss of generality, assume that the errors
  (conformal scores) $s_i$ are sorted. Then the probability that
  $s_\text{test} = 1 - f_{y_\text{test}}(\x_\text{test})$ is less than
  $s_{\ceil{(1+p)(1-\alpha)}}$ is
  $$ \Pro(s_\text{test} \leq s_{\ceil{(1+p)(1-\alpha)}}) =
  \frac{\ceil{(1+p)(1-\alpha)}}{p+1}.
  $$
  Moreover note that the set of events where $s_\text{test} \leq
  s_{\ceil{(1+p)(1-\alpha)}}$ is same as $y_\text{test} \in
  C(\x_\text{test})$. This concludes the result of the theorem.
\end{proof}
\end{document}
