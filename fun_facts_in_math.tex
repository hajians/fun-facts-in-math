\documentclass[11pt]{article}

\usepackage{sectsty}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{amsmath}
\usepackage{amsfonts}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Pro}{\mathbb{P}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\q}{\textsf{q}}
\newcommand{\Lsp}{\mathrm{L}}
\newcommand{\dx}{\text{d}x}
\newcommand{\norm}[1]{\Vert #1 \Vert}

% Margins
%% \topmargin=-0.45in
%% \evensidemargin=0in
%% \oddsidemargin=0in
%% \textwidth=6.5in
%% \textheight=9.0in
%% \headsep=0.25in

\title{Fun facts in mathematics}
\author{Soheil Hajian}
\date{\today}

\begin{document}
\maketitle	
% Optional TOC
\tableofcontents
\pagebreak

%--Paper--
\section{(Numerical) linear algebra}
\subsection{Rayleigh quotient} \label{sec:rayleigh}
\section{Probability theory}
\subsection{Chebyshev inequality}
Let $X$ be a continuous random variable with density function
$f_X(x)$, mean $\mu$ and standard deviation $\sigma$. Then the
following inequality holds
\begin{equation} \label{eq:chebyshev-ineq}
  \Pro(|X-\mu| \geq k \sigma) \leq \frac{1}{k^2}, \quad \forall k \geq
  1.
\end{equation}
In order to prove (\ref{eq:chebyshev-ineq}) we use definition of
the probability and standard deviation of $X$:
\begin{equation}
  \begin{array}{rcl}
    \Pro(|X-\mu| \geq k \sigma) &=& \int_{|x-\mu|\geq k\sigma} f_X(x)
    \dx
    \\
    &\leq& \int_{|x-\mu|\geq k\sigma} \frac{|x-\mu|^2}{(k\sigma)^2}
    f_X(x) \dx
    \\
    &\leq& \int_{\R} \frac{|x-\mu|^2}{(k\sigma)^2}
    f_X(x) \dx
    \\
    &=& \frac{\sigma^2}{(k\sigma)^2}
    \\ &=& \frac{1}{k^2}.
  \end{array}
\end{equation}
%
\subsection{Law of large numbers}
Let us consider a sequence of independent and identically distributed
(i.i.d.) samples $(X_1, X_2, \dots, X_n)$ from a common random
variable $X$. The law of large numbers states that the mean of the
above sequence converges to the mean of $X$ as $n$ grows to
infinity. That is
\begin{equation}
  \bar{X}_n \rightarrow \mu \text{ as }
  n \rightarrow \infty,
\end{equation}
where
\begin{equation}
  \bar{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n},
\end{equation}
and $\mu = \mathbb{E}(X)$. Convergence should be understood in the
sense of almost surely (a.s.) which corresponds to the strong law of
large numbers. The weak law of large numbers corresponds to the
convergence in probability of the mean of the samples to the mean of
$X$.

Here we prove the weak law of large numbers for the case when $X$ is
continuous random variable. From Chebyshev inequality
(\ref{eq:chebyshev-ineq}) we have for the random variable $\bar{X}_n$
\begin{equation} 
  \Pro(|\bar{X}_n-\mu| \geq \varepsilon) \leq
  \frac{\sigma_{\bar{X}_n}^2}{\varepsilon^2},
\end{equation}
where we set $\varepsilon = k \sigma_{\bar{X}_n}$. On the other hand
from the definition of $\bar{X}_i$ we can conclude that
\begin{equation*}
  \sigma_{\bar{X}_n} = \frac{1}{\sqrt{n}} \sigma_X.
\end{equation*}
If $\sigma_X$ is finite we can conclude that 
\begin{equation*} 
  \Pro(|\bar{X}_n-\mu| \geq \varepsilon) \leq
  \frac{\sigma_{X}^2}{n \varepsilon^2},
\end{equation*}
and therefore
\begin{equation} 
  1- \frac{\sigma_{X}^2}{n \varepsilon^2} \leq
  \Pro(|\bar{X}_n-\mu| \leq \varepsilon) \leq
  1, \quad \forall \varepsilon > 0.
\end{equation}
Letting $n \rightarrow 0$ yields that $ \Pro(|\bar{X}_n-\mu| \leq
\varepsilon) \rightarrow 1$ for all $\varepsilon > 0$.

\section{Pattern recognition}
\subsection{Linear discriminant analysis}
In this section we introduce a method to find a subspace that that aims to maximize the distance between data points belonging to different classes (intra-classes distance) while minimizing the distance of data points belonging to the same class (inter-classes).

Let us denote the dataset by the tuple $(X, y)$ where $X \in \R^{n \times p}$ is the so-called feature matrix, and $y \in \R^{n}$ is the target vector. Here $p$ denotes the number of data points in the dataset and $n$ is the dimension of the feature space. 

Each column of $X$ corresponds to the features of a data point which we denote by $x^{(j)} \in \R^{n}$, and similarly each entry of $y$, i.e., $y^{(j)}$ corresponds to the class of the data point $j$, for all $j=1,\dots, p$. We consider that each data point can belong to one and only one class, and we denote the total number of classes by $K$. That is $y^{(j)} \in \{1,\dots, K\}$ for all $j=1,\dots, p$.


\subsubsection{Scatter matrices and spread}
Let us denote the projection of the features of a data point, $x^{(j)}$, into a normalized vector $\q \in \R^{n}$ by $z^{(j)}$, and for all data points by $z := \q^\top X$. A measure of the spread of projected values can be define by the Euclidean $2$-norm, $\norm{z}_2$:
\begin{equation}
	\norm{z}_2 = \q^\top X X^\top \q.
\end{equation}
The matrix $X^\top X$ is called the scatter matrix and we denote it by $S \in \R^{n\times n}$. Note that from the definition of $S$ we can conclude that $S$ is symmetric and at least positive semi-definite. Therefore maximizing the spread corresponds to finding the normalized vector $\q$ that maximizes $\q^\top S \q$, which by Rayleigh quotient (see Section \ref{sec:rayleigh}) corresponds to the eigenvector of $S$ corresponding to the maximum eigenvalue of $S$.

We will now define two metrics: intra-class and inter-class spread of the dataset. It is convenient to define first the set of data points that belong to the same class. Let us define the set of data points belonging to the same class by
\begin{equation}
	\D_k := \big\{ j : y^{(j)} = k, \quad \forall j=1,\dots,p \big\},
\end{equation}
for all $k = 1, \dots, K$. This then motivates to rearrange the feature matrix, $X$, such that data points belonging to the same class be adjacent to each other:
\begin{equation}
	X = [X_1, X_2, \dots, X_K],
\end{equation}
and similarly the target vector $y = (y_1, \dots, y_K)^\top$. Note that each matrix $X_k$ can have a different size, i.e.,  $X_k \in \R^{n \times p_k}$ for all $k = 1, \dots, K$.

In order to define the inter-class spread, we first translate each class the data points to the origin. The center of a class is defined by
\begin{equation}
	c_k := \frac{1}{|\D_k|} \sum_{x \in \D_k} x.
\end{equation}
We then define the centered feature matrix of each class by
\begin{equation}
	X_{k, c} := \Big[x^{(j_1)}-c_k, \quad x^{(j_2)}-c_k, \quad \dots, 
	\quad x^{(j_{p_k})}-c_k \Big],
\end{equation}
and the centered feature matrix by $X_w = [X_{1, c}, \dots, X_{K, c}]$.
The inter-class spread matrix is then defined by $S_w := X_w^{} X_w^{\top}$.
\end{document}
